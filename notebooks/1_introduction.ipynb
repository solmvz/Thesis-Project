{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Node Classification with DGL\n",
        "============================\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import dgl.data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "Number of categories: 7\n"
          ]
        }
      ],
      "source": [
        "dataset = dgl.data.CoraGraphDataset()\n",
        "print(f\"Number of categories: {dataset.num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g = dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "writer = SummaryWriter()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A DGL graph can store node features and edge features in two\n",
        "dictionary-like attributes called ``ndata`` and ``edata``.\n",
        "In the DGL Cora dataset, the graph contains the following node features:\n",
        "\n",
        "- ``train_mask``: A boolean tensor indicating whether the node is in the\n",
        "  training set.\n",
        "\n",
        "- ``val_mask``: A boolean tensor indicating whether the node is in the\n",
        "  validation set.\n",
        "\n",
        "- ``test_mask``: A boolean tensor indicating whether the node is in the\n",
        "  test set.\n",
        "\n",
        "- ``label``: The ground truth node category.\n",
        "\n",
        "-  ``feat``: The node features.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node features\n",
            "{'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
            "Edge features\n",
            "{}\n"
          ]
        }
      ],
      "source": [
        "print(\"Node features\")\n",
        "print(g.ndata)\n",
        "print(\"Edge features\")\n",
        "print(g.edata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:407: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if (graph.in_degrees() == 0).any():\n",
            "c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\heterograph.py:2926: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
            "  if len(vid_tensor) > 0 and F.as_scalar(F.min(vid_tensor, 0)) < 0 < len(\n",
            "c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:57: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return data.item()\n",
            "c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\heterograph.py:3751: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
            "  ) != len(u_tensor):\n",
            "c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\heterograph.py:4343: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if nfeats != num_nodes:\n",
            "c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:455: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if input.numel() > 0:\n"
          ]
        },
        {
          "ename": "TracingCheckError",
          "evalue": "Tracing failed sanity checks!\nERROR: Graphs differed across invocations!\n\tGraph diff:\n\t\t  graph(%self.1 : __torch__.GCN,\n\t\t        %in_feat : Tensor):\n\t\t    %conv2 : __torch__.dgl.nn.pytorch.conv.graphconv.GraphConv = prim::GetAttr[name=\"conv2\"](%self.1)\n\t\t    %conv1 : __torch__.dgl.nn.pytorch.conv.graphconv.GraphConv = prim::GetAttr[name=\"conv1\"](%self.1)\n\t\t    %13 : int = prim::Constant[value=0](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:433:0\n\t\t    %14 : float = prim::Constant[value=-0.5](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:430:0\n\t\t    %15 : int = prim::Constant[value=1](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %16 : NoneType = prim::Constant(), scope: __module.conv1\n\t\t    %17 : bool = prim::Constant[value=0](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %18 : int = prim::Constant[value=6](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %19 : Device = prim::Constant[value=\"cpu\"](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %20 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %input.1 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %input.21 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %input.19 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %input.17 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %input.15 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%conv1)\n\t\t    %weight.5 : Tensor = prim::GetAttr[name=\"weight\"](%conv1)\n\t\t    %28 : Tensor = aten::to(%20, %19, %18, %17, %17, %16), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %degs.1 : Tensor = aten::clamp(%28, %15, %16), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %norm.1 : Tensor = aten::pow(%degs.1, %14), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:430:0\n\t\t    %31 : int = aten::size(%norm.1, %13), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:433:0\n\t\t    %32 : int[] = prim::ListConstruct(%31, %15), scope: __module.conv1\n\t\t    %norm.3 : Tensor = aten::reshape(%norm.1, %32), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:434:0\n\t\t    %feat_src.1 : Tensor = aten::mul(%in_feat, %norm.3), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:435:0\n\t\t    %input.7 : Tensor = aten::matmul(%feat_src.1, %weight.5), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:450:0\n\t\t-   %input.11 : Tensor = ^GSpMM[inplace=0, module=\"dgl.backend.pytorch.sparse\", Subgraph=<Graph>](<dgl.heterograph_index.HeteroGraphIndex object at 0x000001DAB958DA10>, copy_lhs, sum, None)(%input.7), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:553:0\n\t\t?                                                                                                                                                                ^^^^\n\t\t+   %input.11 : Tensor = ^GSpMM[inplace=0, module=\"dgl.backend.pytorch.sparse\", Subgraph=<Graph>](<dgl.heterograph_index.HeteroGraphIndex object at 0x000001DAB954B190>, copy_lhs, sum, None)(%input.7), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:553:0\n\t\t?                                                                                                                                                                ^^^^\n\t\t    %37 : Tensor = aten::to(%20, %19, %18, %17, %17, %16), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:463:0\n\t\t    %degs.3 : Tensor = aten::clamp(%37, %15, %16), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:463:0\n\t\t    %norm.5 : Tensor = aten::pow(%degs.3, %14), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:465:0\n\t\t    %40 : int = aten::size(%norm.5, %13), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:468:0\n\t\t    %41 : int[] = prim::ListConstruct(%40, %15), scope: __module.conv1\n\t\t    %norm.7 : Tensor = aten::reshape(%norm.5, %41), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:469:0\n\t\t    %rst.1 : Tensor = aten::mul(%input.11, %norm.7), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:470:0\n\t\t    %input.13 : Tensor = aten::add(%rst.1, %bias.1, %15), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:473:0\n\t\t    %45 : (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) = prim::TupleConstruct(%input.13, %input.15, %input.17, %input.19, %input.21, %input.1)\n\t\t    %5 : Tensor, %6 : Tensor, %7 : Tensor, %8 : Tensor, %9 : Tensor, %10 : Tensor = prim::TupleUnpack(%45)\n\t\t    %feat : Tensor = aten::relu(%5) # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1473:0\n\t\t    %46 : int = prim::Constant[value=0](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:433:0\n\t\t    %47 : float = prim::Constant[value=-0.5](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:430:0\n\t\t    %48 : int = prim::Constant[value=1](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %49 : NoneType = prim::Constant(), scope: __module.conv2\n\t\t    %50 : bool = prim::Constant[value=0](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %51 : int = prim::Constant[value=6](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %52 : Device = prim::Constant[value=\"cpu\"](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %53 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %bias : Tensor = prim::GetAttr[name=\"bias\"](%conv2)\n\t\t    %weight : Tensor = prim::GetAttr[name=\"weight\"](%conv2)\n\t\t    %56 : Tensor = aten::to(%53, %52, %51, %50, %50, %49), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %degs.5 : Tensor = aten::clamp(%56, %48, %49), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %norm.9 : Tensor = aten::pow(%degs.5, %47), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:430:0\n\t\t    %59 : int = aten::size(%norm.9, %46), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:433:0\n\t\t    %60 : int[] = prim::ListConstruct(%59, %48), scope: __module.conv2\n\t\t    %norm.11 : Tensor = aten::reshape(%norm.9, %60), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:434:0\n\t\t    %feat_src : Tensor = aten::mul(%feat, %norm.11), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:435:0\n\t\t    %input.27 : Tensor = aten::matmul(%feat_src, %weight), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:450:0\n\t\t-   %input : Tensor = ^GSpMM[inplace=0, module=\"dgl.backend.pytorch.sparse\", Subgraph=<Graph>](<dgl.heterograph_index.HeteroGraphIndex object at 0x000001DABC770650>, copy_lhs, sum, None)(%input.27), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:553:0\n\t\t?                                                                                                                                                             ^^^^\n\t\t+   %input : Tensor = ^GSpMM[inplace=0, module=\"dgl.backend.pytorch.sparse\", Subgraph=<Graph>](<dgl.heterograph_index.HeteroGraphIndex object at 0x000001DAB94C37D0>, copy_lhs, sum, None)(%input.27), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:553:0\n\t\t?                                                                                                                                                           ++ + ^\n\t\t    %65 : Tensor = aten::to(%53, %52, %51, %50, %50, %49), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:463:0\n\t\t    %degs : Tensor = aten::clamp(%65, %48, %49), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:463:0\n\t\t    %norm.13 : Tensor = aten::pow(%degs, %47), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:465:0\n\t\t    %68 : int = aten::size(%norm.13, %46), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:468:0\n\t\t    %69 : int[] = prim::ListConstruct(%68, %48), scope: __module.conv2\n\t\t    %norm : Tensor = aten::reshape(%norm.13, %69), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:469:0\n\t\t    %rst : Tensor = aten::mul(%input, %norm), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:470:0\n\t\t    %72 : Tensor = aten::add(%rst, %bias, %48), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:473:0\n\t\t    return (%72)\n\tFirst diverging operator:\n\tNode diff:\n\t\t- %conv2 : __torch__.dgl.nn.pytorch.conv.graphconv.___torch_mangle_80.GraphConv = prim::GetAttr[name=\"conv2\"](%self.1)\n\t\t?                                                                   ^\n\t\t+ %conv2 : __torch__.dgl.nn.pytorch.conv.graphconv.___torch_mangle_83.GraphConv = prim::GetAttr[name=\"conv2\"](%self.1)\n\t\t?                                                                   ^\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTracingCheckError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[65], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m GCN(feature_dim, dataset\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[0;32m     22\u001b[0m sample_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(g\u001b[38;5;241m.\u001b[39mnumber_of_nodes(), feature_dim)\n\u001b[1;32m---> 23\u001b[0m tranced_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mtrace(model, sample_features)\n\u001b[0;32m     25\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_graph(tranced_model, sample_features)\n",
            "File \u001b[1;32mc:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\jit\\_trace.py:806\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    805\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trace_module(\n\u001b[0;32m    807\u001b[0m         func,\n\u001b[0;32m    808\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_inputs},\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    810\u001b[0m         check_trace,\n\u001b[0;32m    811\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[0;32m    812\u001b[0m         check_tolerance,\n\u001b[0;32m    813\u001b[0m         strict,\n\u001b[0;32m    814\u001b[0m         _force_outplace,\n\u001b[0;32m    815\u001b[0m         _module_class,\n\u001b[0;32m    816\u001b[0m         example_inputs_is_kwarg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(example_kwarg_inputs, \u001b[38;5;28mdict\u001b[39m),\n\u001b[0;32m    817\u001b[0m         _store_inputs\u001b[38;5;241m=\u001b[39m_store_inputs,\n\u001b[0;32m    818\u001b[0m     )\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    823\u001b[0m ):\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\jit\\_trace.py:1102\u001b[0m, in \u001b[0;36mtrace_module\u001b[1;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[0;32m   1090\u001b[0m                 _check_trace(\n\u001b[0;32m   1091\u001b[0m                     check_inputs,\n\u001b[0;32m   1092\u001b[0m                     func,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1099\u001b[0m                     example_inputs_is_kwarg\u001b[38;5;241m=\u001b[39mexample_inputs_is_kwarg,\n\u001b[0;32m   1100\u001b[0m                 )\n\u001b[0;32m   1101\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1102\u001b[0m                 _check_trace(\n\u001b[0;32m   1103\u001b[0m                     [inputs],\n\u001b[0;32m   1104\u001b[0m                     func,\n\u001b[0;32m   1105\u001b[0m                     check_trace_method,\n\u001b[0;32m   1106\u001b[0m                     check_tolerance,\n\u001b[0;32m   1107\u001b[0m                     strict,\n\u001b[0;32m   1108\u001b[0m                     _force_outplace,\n\u001b[0;32m   1109\u001b[0m                     \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1110\u001b[0m                     _module_class,\n\u001b[0;32m   1111\u001b[0m                     example_inputs_is_kwarg\u001b[38;5;241m=\u001b[39mexample_inputs_is_kwarg,\n\u001b[0;32m   1112\u001b[0m                 )\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1114\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;241m=\u001b[39m old_module_map\n",
            "File \u001b[1;32mc:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\jit\\_trace.py:577\u001b[0m, in \u001b[0;36m_check_trace\u001b[1;34m(check_inputs, func, traced_func, check_tolerance, strict, force_outplace, is_trace_module, _module_class, example_inputs_is_kwarg)\u001b[0m\n\u001b[0;32m    575\u001b[0m diag_info \u001b[38;5;241m=\u001b[39m graph_diagnostic_info()\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m diag_info):\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TracingCheckError(\u001b[38;5;241m*\u001b[39mdiag_info)\n",
            "\u001b[1;31mTracingCheckError\u001b[0m: Tracing failed sanity checks!\nERROR: Graphs differed across invocations!\n\tGraph diff:\n\t\t  graph(%self.1 : __torch__.GCN,\n\t\t        %in_feat : Tensor):\n\t\t    %conv2 : __torch__.dgl.nn.pytorch.conv.graphconv.GraphConv = prim::GetAttr[name=\"conv2\"](%self.1)\n\t\t    %conv1 : __torch__.dgl.nn.pytorch.conv.graphconv.GraphConv = prim::GetAttr[name=\"conv1\"](%self.1)\n\t\t    %13 : int = prim::Constant[value=0](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:433:0\n\t\t    %14 : float = prim::Constant[value=-0.5](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:430:0\n\t\t    %15 : int = prim::Constant[value=1](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %16 : NoneType = prim::Constant(), scope: __module.conv1\n\t\t    %17 : bool = prim::Constant[value=0](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %18 : int = prim::Constant[value=6](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %19 : Device = prim::Constant[value=\"cpu\"](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %20 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %input.1 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %input.21 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %input.19 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %input.17 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %input.15 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:88:0\n\t\t    %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%conv1)\n\t\t    %weight.5 : Tensor = prim::GetAttr[name=\"weight\"](%conv1)\n\t\t    %28 : Tensor = aten::to(%20, %19, %18, %17, %17, %16), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %degs.1 : Tensor = aten::clamp(%28, %15, %16), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %norm.1 : Tensor = aten::pow(%degs.1, %14), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:430:0\n\t\t    %31 : int = aten::size(%norm.1, %13), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:433:0\n\t\t    %32 : int[] = prim::ListConstruct(%31, %15), scope: __module.conv1\n\t\t    %norm.3 : Tensor = aten::reshape(%norm.1, %32), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:434:0\n\t\t    %feat_src.1 : Tensor = aten::mul(%in_feat, %norm.3), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:435:0\n\t\t    %input.7 : Tensor = aten::matmul(%feat_src.1, %weight.5), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:450:0\n\t\t-   %input.11 : Tensor = ^GSpMM[inplace=0, module=\"dgl.backend.pytorch.sparse\", Subgraph=<Graph>](<dgl.heterograph_index.HeteroGraphIndex object at 0x000001DAB958DA10>, copy_lhs, sum, None)(%input.7), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:553:0\n\t\t?                                                                                                                                                                ^^^^\n\t\t+   %input.11 : Tensor = ^GSpMM[inplace=0, module=\"dgl.backend.pytorch.sparse\", Subgraph=<Graph>](<dgl.heterograph_index.HeteroGraphIndex object at 0x000001DAB954B190>, copy_lhs, sum, None)(%input.7), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:553:0\n\t\t?                                                                                                                                                                ^^^^\n\t\t    %37 : Tensor = aten::to(%20, %19, %18, %17, %17, %16), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:463:0\n\t\t    %degs.3 : Tensor = aten::clamp(%37, %15, %16), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:463:0\n\t\t    %norm.5 : Tensor = aten::pow(%degs.3, %14), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:465:0\n\t\t    %40 : int = aten::size(%norm.5, %13), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:468:0\n\t\t    %41 : int[] = prim::ListConstruct(%40, %15), scope: __module.conv1\n\t\t    %norm.7 : Tensor = aten::reshape(%norm.5, %41), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:469:0\n\t\t    %rst.1 : Tensor = aten::mul(%input.11, %norm.7), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:470:0\n\t\t    %input.13 : Tensor = aten::add(%rst.1, %bias.1, %15), scope: __module.conv1 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:473:0\n\t\t    %45 : (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) = prim::TupleConstruct(%input.13, %input.15, %input.17, %input.19, %input.21, %input.1)\n\t\t    %5 : Tensor, %6 : Tensor, %7 : Tensor, %8 : Tensor, %9 : Tensor, %10 : Tensor = prim::TupleUnpack(%45)\n\t\t    %feat : Tensor = aten::relu(%5) # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1473:0\n\t\t    %46 : int = prim::Constant[value=0](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:433:0\n\t\t    %47 : float = prim::Constant[value=-0.5](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:430:0\n\t\t    %48 : int = prim::Constant[value=1](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %49 : NoneType = prim::Constant(), scope: __module.conv2\n\t\t    %50 : bool = prim::Constant[value=0](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %51 : int = prim::Constant[value=6](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %52 : Device = prim::Constant[value=\"cpu\"](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %53 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %bias : Tensor = prim::GetAttr[name=\"bias\"](%conv2)\n\t\t    %weight : Tensor = prim::GetAttr[name=\"weight\"](%conv2)\n\t\t    %56 : Tensor = aten::to(%53, %52, %51, %50, %50, %49), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %degs.5 : Tensor = aten::clamp(%56, %48, %49), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:428:0\n\t\t    %norm.9 : Tensor = aten::pow(%degs.5, %47), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:430:0\n\t\t    %59 : int = aten::size(%norm.9, %46), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:433:0\n\t\t    %60 : int[] = prim::ListConstruct(%59, %48), scope: __module.conv2\n\t\t    %norm.11 : Tensor = aten::reshape(%norm.9, %60), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:434:0\n\t\t    %feat_src : Tensor = aten::mul(%feat, %norm.11), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:435:0\n\t\t    %input.27 : Tensor = aten::matmul(%feat_src, %weight), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:450:0\n\t\t-   %input : Tensor = ^GSpMM[inplace=0, module=\"dgl.backend.pytorch.sparse\", Subgraph=<Graph>](<dgl.heterograph_index.HeteroGraphIndex object at 0x000001DABC770650>, copy_lhs, sum, None)(%input.27), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:553:0\n\t\t?                                                                                                                                                             ^^^^\n\t\t+   %input : Tensor = ^GSpMM[inplace=0, module=\"dgl.backend.pytorch.sparse\", Subgraph=<Graph>](<dgl.heterograph_index.HeteroGraphIndex object at 0x000001DAB94C37D0>, copy_lhs, sum, None)(%input.27), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:553:0\n\t\t?                                                                                                                                                           ++ + ^\n\t\t    %65 : Tensor = aten::to(%53, %52, %51, %50, %50, %49), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:463:0\n\t\t    %degs : Tensor = aten::clamp(%65, %48, %49), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:463:0\n\t\t    %norm.13 : Tensor = aten::pow(%degs, %47), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:465:0\n\t\t    %68 : int = aten::size(%norm.13, %46), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:468:0\n\t\t    %69 : int[] = prim::ListConstruct(%68, %48), scope: __module.conv2\n\t\t    %norm : Tensor = aten::reshape(%norm.13, %69), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:469:0\n\t\t    %rst : Tensor = aten::mul(%input, %norm), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:470:0\n\t\t    %72 : Tensor = aten::add(%rst, %bias, %48), scope: __module.conv2 # c:\\Users\\solma\\anaconda3\\Lib\\site-packages\\dgl\\nn\\pytorch\\conv\\graphconv.py:473:0\n\t\t    return (%72)\n\tFirst diverging operator:\n\tNode diff:\n\t\t- %conv2 : __torch__.dgl.nn.pytorch.conv.graphconv.___torch_mangle_80.GraphConv = prim::GetAttr[name=\"conv2\"](%self.1)\n\t\t?                                                                   ^\n\t\t+ %conv2 : __torch__.dgl.nn.pytorch.conv.graphconv.___torch_mangle_83.GraphConv = prim::GetAttr[name=\"conv2\"](%self.1)\n\t\t?                                                                   ^\n"
          ]
        }
      ],
      "source": [
        "from dgl.nn import GraphConv\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, 16)\n",
        "        self.conv2 = GraphConv(16, num_classes)\n",
        "\n",
        "    def forward(self, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "# Create the model with given dimensions\n",
        "#features = torch.randn(g.number_of_nodes(), g.ndata[\"feat\"].shape[1])\n",
        "feature_dim = g.ndata['feat'].shape[1]\n",
        "model = GCN(feature_dim, dataset.num_classes)\n",
        "\n",
        "sample_features = torch.randn(g.number_of_nodes(), feature_dim)\n",
        "tranced_model = torch.jit.trace(model, sample_features)\n",
        "\n",
        "writer.add_graph(tranced_model, sample_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DGL provides implementation of many popular neighbor aggregation\n",
        "modules. You can easily invoke them with one line of code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the GCN\n",
        "----------------\n",
        "\n",
        "Training this GCN is similar to training other PyTorch neural networks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In epoch 0, loss: 1.947, val acc: 0.110 (best 0.110), test acc: 0.110 (best 0.110)\n",
            "In epoch 5, loss: 1.907, val acc: 0.348 (best 0.424), test acc: 0.349 (best 0.451)\n",
            "In epoch 10, loss: 1.836, val acc: 0.476 (best 0.476), test acc: 0.515 (best 0.515)\n",
            "In epoch 15, loss: 1.741, val acc: 0.540 (best 0.540), test acc: 0.577 (best 0.577)\n",
            "In epoch 20, loss: 1.619, val acc: 0.604 (best 0.604), test acc: 0.622 (best 0.622)\n",
            "In epoch 25, loss: 1.474, val acc: 0.678 (best 0.678), test acc: 0.690 (best 0.690)\n",
            "In epoch 30, loss: 1.310, val acc: 0.710 (best 0.710), test acc: 0.712 (best 0.712)\n",
            "In epoch 35, loss: 1.134, val acc: 0.726 (best 0.726), test acc: 0.734 (best 0.734)\n",
            "In epoch 40, loss: 0.956, val acc: 0.742 (best 0.742), test acc: 0.755 (best 0.749)\n",
            "In epoch 45, loss: 0.787, val acc: 0.748 (best 0.750), test acc: 0.762 (best 0.758)\n",
            "In epoch 50, loss: 0.637, val acc: 0.754 (best 0.754), test acc: 0.766 (best 0.769)\n",
            "In epoch 55, loss: 0.509, val acc: 0.754 (best 0.756), test acc: 0.772 (best 0.770)\n",
            "In epoch 60, loss: 0.406, val acc: 0.752 (best 0.756), test acc: 0.778 (best 0.770)\n",
            "In epoch 65, loss: 0.324, val acc: 0.758 (best 0.758), test acc: 0.775 (best 0.775)\n",
            "In epoch 70, loss: 0.261, val acc: 0.764 (best 0.764), test acc: 0.772 (best 0.771)\n",
            "In epoch 75, loss: 0.211, val acc: 0.762 (best 0.764), test acc: 0.768 (best 0.771)\n",
            "In epoch 80, loss: 0.174, val acc: 0.772 (best 0.772), test acc: 0.767 (best 0.767)\n",
            "In epoch 85, loss: 0.144, val acc: 0.772 (best 0.774), test acc: 0.766 (best 0.767)\n",
            "In epoch 90, loss: 0.121, val acc: 0.772 (best 0.774), test acc: 0.767 (best 0.767)\n",
            "In epoch 95, loss: 0.103, val acc: 0.768 (best 0.774), test acc: 0.768 (best 0.767)\n"
          ]
        }
      ],
      "source": [
        "def train(g, model):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    features = g.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "    train_mask = g.ndata[\"train_mask\"]\n",
        "    val_mask = g.ndata[\"val_mask\"]\n",
        "    test_mask = g.ndata[\"test_mask\"]\n",
        "    for e in range(100):\n",
        "        # Forward\n",
        "        logits = model(g, features)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        # Compute loss\n",
        "        # Note that you should only compute the losses of the nodes in the training set.\n",
        "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "\n",
        "        # Compute accuracy on training/validation/test\n",
        "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
        "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
        "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
        "\n",
        "        # Save the best validation accuracy and the corresponding test accuracy.\n",
        "        if best_val_acc < val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            print(\n",
        "                f\"In epoch {e}, loss: {loss:.3f}, val acc: {val_acc:.3f} (best {best_val_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f})\"\n",
        "            )\n",
        "\n",
        "\n",
        "model = GCN(g.ndata[\"feat\"].shape[1], dataset.num_classes)\n",
        "train(g, model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
